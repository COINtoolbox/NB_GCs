
---
title: "GCS analysis"
author: "Bart Buelens, Rafael S. de Souza"
date: "December 8, 2014"
output: pdf_document
---

This document contains some preliminary work possibly of use to a paper by the COIN group on negative binomial distributions to model count data.

It is put together in RStudio as a Rmd file from which a PDF is easily generated.

# Introduction
The catalog of Globular Cluster Systems (GCS) presented in Harris et al. 2013 is analyzed. In particular, three statistical models relating the number of GCSs to the dynamical mass of galaxies are studied. These include a linear model assuming gaussianity, and generalized linear models assuming poisson and negative binomial distributions.
At the end the analysis is repeated using the black hole mass instead of the dynamical mass.


# Before proceding some useful libraries are loaded.
``` {r, results='hide',message=FALSE, cache=FALSE}
library(ggplot2)
library(MASS)     # glm.nb
library(COUNT)   # diagnostics
library(lmtest) # Likelihood test
library(gamlss.tr)# zero truncated model
#library(glmmADMB)  # GLMM
library(caret) # 10-fold
library(reshape) # melt
library(scales)
require(ggthemes)
```


# Data
The data set 'GCs.csv' sent by e-mail by Rafael on Dec 1st is used.
``` {r Read data}
GCS = read.csv(file="..//Dataset//GCs.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(Mdyn)) # 1 removed
dim(GCS)
```
Of the initial 422 galaxies in the catalog, 45 are retained for which variables needed here are present. The errors are used further down. Weights are defined to be the inverse of the error.
``` {r Weights}
GCS$w = 1/ GCS$N_GC_err

```
It is somewhat strange that in the catalogue the GCS count is not always integer; an integer version is derived here simply by rounding.
* NOTE: in Rafas file these are integer! *


# Data exploration

```{r,fig.width=7, fig.height=6}

ggplot(GCS,aes(x=MBH,y=N_GC,colour=Type,shape=Type))+geom_point(size=3)+
scale_y_continuous(trans = 'log10',
                     breaks=trans_breaks("log10",function(x) 10^x),
                     labels=trans_format("log10",math_format(10^.x)))+
  scale_colour_tableau()+scale_shape_tremmel()+
  theme_economist_white(gray_bg = F, base_size = 11, base_family = "sans")+
  ylab(expression(N[GC]))+xlab(expression(M[BH]))
````


# Modeling
The first way to model counts is using a linear model for the log counts. The second is a glm poisson model, the third is a glm with negative binomial. These are specified and fitted as follows.
``` {r Fit models}
linmFit = glm(log(N_GC) ~ Mdyn, family="gaussian",GCS)
poisFit = glm(N_GC ~ Mdyn, GCS, family="poisson")
negbFit = glm.nb(N_GC ~ Mdyn, GCS)
```
The results are now plotted using ggplot() from the ggplot2 package.
``` {r}
X = GCS[,c("Galaxy","Mdyn","N_GC")]
X = rbind(X,X,X,X)
X$method = factor(rep(c("obs","OLS","Poisson","NB"),each=nrow(GCS)))
X[X$method == "OLS","N_GC"] = exp(linmFit$fitted.values)
X[X$method == "Poisson","N_GC"] = poisFit$fitted.values
X[X$method == "NB","N_GC"] = negbFit$fitted.values
ggplot(X, aes(x = Mdyn,y=N_GC)) + 
   scale_y_continuous(trans = 'log10',
                     breaks=trans_breaks("log10",function(x) 10^x),
                     labels=trans_format("log10",math_format(10^.x)))+
  scale_color_solarized()+scale_shape_stata()+
  theme_economist_white(gray_bg = F, base_size = 10, base_family = "sans")+
  theme(legend.text=element_text(size=8),legend.position="right",plot.title = element_text(hjust=0.5),
        axis.title.y=element_text(vjust=0.75),
        axis.title.x=element_text(vjust=-0.25),
        text = element_text(size=10))+
   geom_point(data = subset(X, method=="obs")) + 
  geom_line(aes(colour=method,linetype=method), data = subset(X, method != "obs"))+
  xlab(expression(M[dyn]))+ylab(expression(N[GC]))
  
``` 
The three are not dramatically different. 

# Model comparison
A quantitative comparison is obtained using leave-one-out cross validation (LOO-CV). Each observation is left out once and predicted using a model fit on the other data. The errors are the differences between the predictions and true values. The squared differences are weighted using the weights derived earlier, inversely proportional to the errors on the known values. LOO-CV is implemented as follows.
``` {r "LOO calculation"}
N = nrow(GCS)
A = data.frame(linm=rep(NA,N))
A$negb = A$pois = NA
for (i in 1:N) {
   Gx = GCS[-i,]
   f1 = lm(log(N_GC) ~ Mdyn, Gx)
   f2 = glm(N_GC ~ Mdyn, Gx, family="poisson")
   f3 = glm.nb(N_GC ~ Mdyn, Gx)
   A[i,"linm"] = GCS$w[i] * (GCS$N_GC[i] - exp(predict(f1, GCS[i,])))^2
   A[i, "pois"] = GCS$w[i] * (GCS$N_GC[i] - predict(f2, GCS[i,]))^2   
   A[i, "negb"] = GCS$w[i] * (GCS$N_GC[i] - predict(f3, GCS[i,]))^2   
}
``` 
The square root of the mean of the values in A gives the LOO-CV measure.
``` {r LOO result}
lapply(A, function(x) sqrt(mean(x)))
``` 
Based on the LOO-CV criterion, the linear model is best. In this case there is no benefit in using Poisson or negative binomial models. 




# Black hole mass
The same analysis now using the black hole mass as predictor rather than dynamical mass.

``` {r}
linmFit = lm(log(N_GC) ~ MBH, GCS)
#gaussFit = glm(log(N_GC) ~ MBH, family="gaussian",GCS)
poisFit = glm(N_GC ~MBH, GCS, family="poisson")
negbFit = glm.nb(N_GC ~ MBH, GCS)
#negbmFit <- glmmadmb(N_GC~MBH +(1|Type),
#                  data=GCS, family="nbinom")
#gen.trun(0,"NBI",type="left",name="lefttr")
#trunFit=gamlss(N_GC ~ MBH, data=GCS[,c("N_GC","MBH")],family=NBIlefttr)
X = GCS[,c("Galaxy","MBH","N_GC")]
X = rbind(X,X,X,X)
X$method = factor(rep(c("obs","OLS","Poisson","NB"),each=nrow(GCS)))
X[X$method == "OLS","N_GC"] = exp(linmFit$fitted.values)
#X[X$method == "gauss","N_GC"] = exp(gaussFit$fitted.values)
X[X$method == "Poisson","N_GC"] = poisFit$fitted.values
X[X$method == "NB","N_GC"] = negbFit$fitted.values
#X[X$method == "trunnegb","N_GC"] = predict(trunFit, type="response")
ggplot(X, aes(x = MBH,y=N_GC)) + 
  scale_y_continuous(trans = 'log10',
                     breaks=trans_breaks("log10",function(x) 10^x),
                     labels=trans_format("log10",math_format(10^.x)))+
  scale_color_tableau()+scale_linetype_stata()+
  theme_economist_white(gray_bg = F, base_size = 10, base_family = "sans")+
  theme(legend.text=element_text(size=8),legend.position="right",plot.title = element_text(hjust=0.5),
        axis.title.y=element_text(vjust=0.75),
        axis.title.x=element_text(vjust=-0.25),
        text = element_text(size=10))+
   geom_point(data = subset(X, method=="obs")) + 
   geom_line(aes(colour=method,linetype=method), data = subset(X, method != "obs"))+ylab(expression(N[GC]))+
  xlab(expression(log(M[BH]/M[Sun])))
A = data.frame(linm=rep(NA,N))
A$negb = A$pois = A$gauss= NA
for (i in 1:N) {
   Gx = GCS[-i,]
   f1 = lm(log(N_GC) ~ MBH, Gx)
#   f2 = glm(log(N_GC) ~ MBH, family="gaussian",Gx)
   f2 = glm(N_GC ~ MBH, Gx, family="poisson")
   f3 = glm.nb(N_GC ~ MBH, Gx)
   A[i,"linm"] = GCS$w[i] * (GCS$N_GC[i] - exp(predict(f1, GCS[i,],type="response")))^2
#   A[i,"gauss"] = GCS$w[i] * (GCS$N_GC[i] - exp(predict(f2, GCS[i,],type="response")))^2
   A[i, "pois"] = GCS$w[i] * (GCS$N_GC[i] - predict(f2, GCS[i,],type="response"))^2   
   A[i, "negb"] = GCS$w[i] * (GCS$N_GC[i] - predict(f3, GCS[i,],type="response"))^2   
}
lapply(A, function(x) sqrt(mean(x)))
```

# 10-fold cross validation
```{r}
folds <- createFolds(GCS$N_GC, k=10)
A = data.frame(linm=rep(NA,N))
A$negb = A$pois = A$gauss= NA
for (i in 1:10) {
   Gx = GCS[-folds[[i]],]
   f1 = lm(log(N_GC) ~ MBH, Gx)
   f2 = glm(log(N_GC) ~ MBH, family="gaussian",Gx)
   f3 = glm(N_GC ~ MBH, Gx, family="poisson")
   f4 = glm.nb(N_GC ~ MBH, Gx)
   A[folds[[i]],"linm"] = GCS$w[folds[[i]]] * (GCS$N_GC[folds[[i]]] - exp(predict(f1, GCS[folds[[i]],],type="response")))^2
   A[folds[[i]],"gauss"] = GCS$w[folds[[i]]] * (GCS$N_GC[folds[[i]]] - exp(predict(f2, GCS[folds[[i]],],type="response")))^2
   A[folds[[i]], "pois"] = GCS$w[folds[[i]]] * (GCS$N_GC[folds[[i]]] - predict(f3, GCS[folds[[i]],],type="response"))^2   
   A[folds[[i]], "negb"] = GCS$w[folds[[i]]] * (GCS$N_GC[folds[[i]]] - predict(f4, GCS[folds[[i]],],type="response"))^2   
}
lapply(A, function(x) sqrt(mean(x)))
```


# Errors for GLM  Prediction

## Confidence interval

``` {r}
pred_glm<-function(x){
preds <- predict(x, type = "link", se.fit = TRUE)
critval <- 1.96 ## approx 95% CI
upr <- preds$fit + (critval * preds$se.fit)
lwr <- preds$fit - (critval * preds$se.fit)
fit <- preds$fit

fit2 <- x$family$linkinv(fit)
upr2 <- x$family$linkinv(upr)
lwr2 <- x$family$linkinv(lwr)
errup<-upr2-fit2
errlow<-fit2-lwr2
return(list(fit=fit2,lwr=lwr2,upr=upr2,errlow =errlow,errup=errup))
}
```

## Prediction

The above computes a confidence interval. Following code is an attempt to compute prediction intervals: taking into account variance of estimated parameters through bootstrap, and prediction uncertainty due to model variance.

``` {r, cache = TRUE}
N = nrow(GCS)
# predN = 50
# MBHx = data.frame(MBH = seq(from = 0.9 * min(GCS$MBH), 
#                             to = 1.1 * max(GCS$MBH), 
#                             length.out = predN))
# Do the following instead, to enable calculation of coverage.
MBHx = data.frame(MBH = GCS$MBH)
predN = length(GCS$MBH)
bN = 1000
Blinm = Bpois = Bnegb = matrix(data = NA, nrow = bN, ncol = predN)
BlinmU = BlinmL = BpoisU = BpoisL = BnegbU = BnegbL = Blinm
qlwr = 0.025
qupr = 0.975
for (i in 1:bN) {
  G = GCS[sample(1:N, N, replace = TRUE),]
  linmFit = glm(log(N_GC) ~ MBH, G, family="gaussian")
  poisFit = glm(N_GC ~ MBH, G, family="poisson")
  negbFit = glm.nb(N_GC ~ MBH, G)
  linmP = predict(linmFit, MBHx)
  poisP = predict(poisFit, MBHx)
  negbP = predict(negbFit, MBHx)
  Blinm[i,] = linmP
  BlinmL[i,] = qnorm(qlwr, mean = linmP, sd = sd(residuals(linmFit)))
  BlinmU[i,] = qnorm(qupr, mean = linmP, sd = sd(residuals(linmFit)))
  Bpois[i,] = poisP
  BpoisL[i,] = log(qpois(qlwr, lambda = exp(poisP)))
  BpoisU[i,] = log(qpois(qupr, lambda = exp(poisP)))
  Bnegb[i,] = negbP
  BnegbL[i,] = log(qnbinom(qlwr, size = negbFit$theta, mu = exp(negbP)))
  BnegbU[i,] = log(qnbinom(qupr, size = negbFit$theta, mu = exp(negbP)))
} 

# fit once more, on the full sample, as a reference:
linmFit = glm(log(N_GC) ~ MBH, GCS, family="gaussian")
poisFit = glm(N_GC ~ MBH, GCS, family="poisson")
negbFit = glm.nb(N_GC ~ MBH, GCS)
linmPred = predict(linmFit, MBHx)
poisPred = predict(poisFit, MBHx)
negbPred = predict(negbFit, MBHx)

# Compare bootstrap means with reference to check validity of bootstrap:
summary(colMeans(Blinm) - linmPred)
summary(colMeans(Bpois) - poisPred)
summary(colMeans(Bnegb) - negbPred)

# Now plot predicted means and prediction intervals
P = MBHx
P$linm = linmPred
P$pois = poisPred
P$negb = negbPred
P = melt(P, id.vars = "MBH", variable_name = "model")
names(P)[3] = "logN_GC"
P$lwr = c(colMeans(BlinmL), colMeans(BpoisL), colMeans(BnegbL))
P$upr = c(colMeans(BlinmU), colMeans(BpoisU), colMeans(BnegbU))
```

```{r,fig.width=6, fig.height=9}
GCS$logN_GC = log(GCS$N_GC)
ggplot(GCS, aes(x = MBH, y = logN_GC)) + 
#  scale_y_continuous(trans = 'log10',
#                     breaks=trans_breaks("log10",function(x) 10^x),
#                     labels=trans_format("log10",math_format(10^.x)))+
  geom_point() + 
  geom_line(data=P, aes(x = MBH, y=logN_GC, colour=model)) +  
  geom_ribbon(data=P, aes(ymin=lwr, ymax=upr, fill=model), alpha=0.5) + 
  facet_wrap(~ model, ncol=1)+scale_color_tableau()+scale_linetype_stata()+
  scale_fill_tableau()+
  theme_economist_white(gray_bg = T, base_size = 10, base_family = "sans")+
  theme(legend.text=element_text(size=8),legend.position="right",plot.title = element_text(hjust=0.5),
        axis.title.y=element_text(vjust=0.75),
        axis.title.x=element_text(vjust=-0.25),
        text = element_text(size=10))
```

Consider coverage.
``` {r}
P$obs = rep(GCS$logN_GC,3)
P$cov = P$lwr <= P$obs & P$obs <= P$upr
table(P[,c("model","cov")])
aggregate(P$cov, by=list(P$model), FUN = function(x) sum(x)/length(GCS$MBH))
```
Negative binomial has best coverage, as it should be 95%. Poisson is very bad, as it is unable to capture the overdispersion.

# Bias diagnostic
Modeling log counts as a linear model with gaussian error tends to lead to biased results on the actual scale of the counts. Compare the total number of GCS in the sample with the total of the fitted values.

``` {r}
sum(GCS$N_GC)
sum(exp(linmFit$fitted.values)) 
sum(poisFit$fitted.values)
sum(negbFit$fitted.values)
```
We could compute errors on these in the above bootstrap routine.

# Dispersion statistics
````{r}
P_disp<-function(x){
  pr<-sum(residuals(x,type="pearson")^2)  
  dispersion<-pr/x$df.residual
  cat("\n Pearson Chi2 = ",pr,
      "\n Dispersion = ",dispersion, "\n")
}
P_disp(poisFit)
P_disp(negbFit)

````

# Average Marginal effects

How the probability of the count response changes with a one-unit change in the value of the continuous predictor. 

```{r}
mMBHmean<-mean(GCS$MBH)
xb<-coef(negbFit)[1]+coef(negbFit)[2]
dfdxb<-exp(xb)*coef(negbFit)[2]
mean(dfdxb)
````

# Goodness of fit 
Deviance is asymptotically chi squared:

``` {r}
 1 - pchisq(summary(linmFit)$deviance,summary(linmFit)$df.residual)
 1 - pchisq(summary(poisFit)$deviance,summary(poisFit)$df.residual)
 1 - pchisq(summary(negbFit)$deviance,summary(negbFit)$df.residual)
````
The Poisson model is not good. The other two are not rejected based on the deviance assessment. Note that the sample is rather small so care must be taken with asymptotics.

# References

Harris, W.E., Harris, G.L.H., and Alessi, M. 2013, ApJ 772, 82.
 


 