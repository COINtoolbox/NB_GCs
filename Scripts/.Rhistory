# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
xinit
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
isCensored
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dinterval(0,censorLimitVec[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
censorLimitVec
isCensored
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dinterval(MBH[i],censorLimitVec[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
xinit
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dinterval(MBH[i],0);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dinterval(MBH[i],censorLimitVec[[i]]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dinterval(MBH[i],censorLimitVec[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
xinit
Upm
censorLimitVec
xinit
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ runif(0,MBH[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dunif(0,MBH[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
xinit
censorLimitVec
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MBH)) # 1 removed
#UpM<-max(GCS$MBH)
Upm<-GCS$MBH+GCS$upMBH
# Censoring information
isCensored = (GCS$MBH == 0 )
GCS$MBH[isCensored] = NA
#censorLimitVec = rep(UpM, length(GCS$MBH) )
censorLimitVec=Upm
xinit=rep( NA , length(GCS$MBH) )
#xinit[isCensored] = censorLimitVec[isCensored]+1
for(i in 1:sum(isCensored)){
xinit[isCensored][[i]] = round(runif(1,0,censorLimitVec[isCensored ][[i]]),2)
}
xinit
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
err_sig_e<-GCS$err_sig_e
jags.data <- list(
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
isCensored = as.numeric(isCensored),
censorLimitVec = censorLimitVec
)
model.NB.c <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
meanx ~ dgamma(30,3)
varx ~ dgamma(2,1)
for (i in 1:N){
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(5,12)
}
# Likelihood function
for (i in 1:N){
isCensored [i] ~ dunif(0,MBH[i]);
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
}
}"
inits <- list(beta.0=0,beta.1=0,size=0.1,MBH=xinit)
params <- c("beta.0","beta.1","size","prediction.NB","MBHtrue")
jags.neg.c <- jags.model(
data = jags.data,
inits = inits,
textConnection(model.NB.c),
n.chains = 3,
n.adapt=1000
)
isCensored
GCS$MBH[isCensored]
censorLimitVec=Upm
xinit
xinit
GCS$MBH[!isCensored]
