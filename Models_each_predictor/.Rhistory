<<<<<<< Updated upstream
install.packages("assertthat")
install.packages("ggplot2")
install.packages("gsl")
install.packages("gsl",type="source")
data_path<-"/Users/rafael/Dropbox/artigos/Meusartigos/IAA-WGC/GLMs/Simulation/data/"
Biffi_data<-read.table(file=paste(data_path,"Biffi2014.csv",sep=""),
header=TRUE)
Biffi_data$fstar<-Biffi_data$Mstar/Biffi_data$Mdm
Biffi_data$fgas<-Biffi_data$Mgas/Biffi_data$Mdm
Biffi_data[1,]
Biffi_data$star.gas<-Biffi_data$Mstar/Biffi_data$Mgas
summary(Biffi_data)
Biffi_data<-read.table(file=paste(data_path,"Biffi2014.csv",sep=""),
header=TRUE)
#Biffi_data_original<-Biffi_data
# Problem 1: xmol, Z, SFR. SFR is the response variable
#Biffi_data<-Biffi_data[,c("SFR","Xmol","Z")]
Biffi_data$Mstar<-1e10*Biffi_data$Mstar
Biffi_data$SFR<-1e10*Biffi_data$SFR
Biffi_data$fstar<-Biffi_data$Mstar/Biffi_data$Mdm
Biffi_data$fgas<-Biffi_data$Mgas/Biffi_data$Mdm
Biffi_data$star.gas<-Biffi_data$Mstar/Biffi_data$Mgas
summary(Biffi_data)
sd(Biffi_data)
sd(Biffi_data$Mdm)
mean(Biffi_data$Mdm)
sd(Biffi_data$Mdm)
sd(Biffi_data$Mgas)
sd(Biffi_data$Mstar)
sd(Biffi_data$SFR)
sd(Biffi_data$Z)
sd(Biffi_data$xmol)
sd(Biffi_data$Xmol)
sd(Biffi_data$fgas)
sd(Biffi_data$fstar)
sd(Biffi_data$star.gas)
install.packages("boot")
install.packages("arm")
install.packages("reshape2")
install.packages("reshape")
install.packages("plyr")
install.packages("COUNT")
install.packages("MCMCpack")
install.packages("stargazer")
install.packages("pROC")
install.packages("scales")
install.packages("SDMTools")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
exp(5.9)
365/(1+365)
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cdoc <- R84$docvis
### JAGS
### ======================================================
K <- 1
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
cdoc   = R84$cdoc,
K=1,
LogN = log(nrow(R84))
)
sink("GLM.txt")
cat("
model{
#1. Priors
beta.0 ~ dt(0, 1/10^2, 1)
beta.1~dt(0, 1, 1)
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- beta.0+beta.1*cdoc[i]
#    cdoc[i]~dbeta(34,14.5)
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta.0  = 0.1,beta.1=0.1
)  }
params <- c("beta.0","beta.1","LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
J0
1/2.5^2
2.5
1/(2.5^2)
cat("
model{
#1. Priors
beta.0 ~ dt(0, 0.16, 1)
beta.1~dt(0, 0.16, 1)
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- beta.0+beta.1*cdoc[i]
#    cdoc[i]~dbeta(34,14.5)
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta.0  = 0.1,beta.1=0.1
)  }
params <- c("beta.0","beta.1","LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
J0
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(0.00001, K)
)
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cage <- R84$age - mean(R84$age)
R84$cdoc <- R84$docvis - mean(R84$docvis)
X <- model.matrix(~ cdoc + female + kids + cage,
data = R84)
K <- ncol(X)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(0.00001, K)
)
sink("GLM.txt")
diag(0.00001, K)
diag(0.00001, 5)
B0   = diag(0.00001, K)
B0
diag(0.00001, 3)
nu = rep(1,K)
nu[]
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cage <- R84$age - mean(R84$age)
R84$cdoc <- R84$docvis - mean(R84$docvis)
X <- model.matrix(~ cdoc + female + kids + cage,
data = R84)
K <- ncol(X)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(1/(2.5^2), K),
nu = rep(1,K)
)
sink("GLM.txt")
cat("
model{
#1. Priors
#    beta  ~ dmnorm(b0[], B0[,])
beta  ~ dt(b0[], B0[,],nu[])
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- inprod(beta[], X[i,])
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta", "LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = rep(1/(2.5^2), K),
nu = rep(1,K)
)
=======
<<<<<<< HEAD
plotPNGsOfMiniPlots()
# clean up
rm("plotPNGsOfMiniPlots", "Manager")
# done
library(R2jags)
# Create model. Try a basic Poisson first.
# due to n=100, lots of variability in the model.
# I usually use n=10000 for my synthetic models.
x1 <- runif(1000)
x2 <- runif(1000)
eta <- 1 - 0.4 * x1 - 0.4 * x2
mu <- exp(eta)
y <- rpois(1000,lambda=mu)
poi <- glm(y ~ x1 + x2, family = poisson)
summary(poi)
X <- model.matrix(~ x1 + x2)
K <- ncol(X)
win.data1 <- list(
Y    = y,
N    = 1000,
X    = X,
bet0 = rep(0, K),          #Betas count
Bet0 = diag(0.0001, K),   #Betas count
Zeros   = rep(0, 1000)
)
win.data1
########
#Model
sink("GPGLM.txt")
cat("
model{
#Priors regression parameters
beta  ~ dmnorm(bet0[], Bet0[,])
#########
#Likelihood
C <- 10000
phi <- 0
for (i in 1:N) {
#Log likelihood function of Generalized Poisson via zero trick:
Zeros[i] ~ dpois(Zeros.mean[i])
Zeros.mean[i] <- -L[i] + C
l1[i] <- log(theta[i])
l2[i] <- (Y[i] - 1) * log(theta[i] + phi * Y[i])
l3[i] <- (theta[i] + phi * Y[i])
l4[i] <- loggam(Y[i] + 1)
L[i]  <-  l1[i] + l2[i] - l3[i] - l4[i]
theta[i] <- (1 - phi) * exp(eta[i])
eta[i]   <- inprod(X[i,], beta[])
z[i]     <- max(-1, -theta[i] / 4)  #Condition
# theta[i] + phi * Y[i] must be >0..else the log is Inf
zz[i]    <- -theta[i] / Y[i]
}
lb <- max(z[])
lb2 <- min(zz[])
#phi ~ dunif(lb, 0.9999)
}
",fill = TRUE)
sink()
#######
#Inits function
inits1  <- function () {
list(beta  = rnorm(K, 0, 0.1)#,  #Regression parameters
#phi   = 0
)  }
#Parameters to estimate
params1 <- c("beta",
#"phi"
"lb",
"lb2"
)
#Start Gibbs sampler
GP   <- jags(data       = win.data1,
inits      = inits1,
parameters = params1,
model      = "GPGLM.txt",
n.thin     = 10,
n.chains   = 3,
n.burnin   = 5000,
n.iter     = 10000)
K
rnorm(K, 0, 0.01)
inits1  <- function () {
list(beta  = rnorm(K, 0, 0.01)#,  #Regression parameters
#phi   = 0
)  }
#Parameters to estimate
params1 <- c("beta",
#"phi"
"lb",
"lb2"
)
#Start Gibbs sampler
GP   <- jags(data       = win.data1,
inits      = inits1,
parameters = params1,
model      = "GPGLM.txt",
n.thin     = 10,
n.chains   = 3,
n.burnin   = 5000,
n.iter     = 10000)
library(R2jags)
x1 <- runif(1000)
x2 <- runif(1000)
eta <- 1 - 0.4 * x1 - 0.4 * x2
mu <- exp(eta)
y <- rpois(1000,lambda=mu)
T1 <- glm(y ~ x1 + x2, family = poisson)
summary(T1)
X <- model.matrix(~ x1 + x2)
K <- ncol(X)
win.data1 <- list(
Y    = y,
N    = 1000,
X    = X,
bet0 = rep(0, K),          #Betas count
Bet0 = diag(0.0001, K),   #Betas count
Zeros   = rep(0, 1000)
)
#Model
sink("GPGLM.txt")
cat("
model{
#Priors regression parameters
beta  ~ dmnorm(bet0[], Bet0[,])
#Likelihood
C <- 10000
phi <- 0
for (i in 1:N) {
#Log likelihood function of Generalized Poisson via zero trick:
Zeros[i] ~ dpois(Zeros.mean[i])
Zeros.mean[i] <- -L[i] + C
l1[i] <- log(theta[i])
l2[i] <- (Y[i] - 1) * log(theta[i] + phi * Y[i])
l3[i] <- (theta[i] + phi * Y[i])
l4[i] <- loggam(Y[i] + 1)
L[i]  <-  l1[i] + l2[i] - l3[i] - l4[i]
theta[i] <- (1 - phi) * exp(eta[i])
eta[i]   <- inprod(X[i,], beta[])
z[i]     <- max(-1, -theta[i] / 4)  #Condition from paper
zz[i]    <- -Y[i] /theta[i]         #theta[i] + phi * Y[i] must be >0..else the log is Inf
}
lb <- max(z[])       #lower bound???
lb2 <- min(zz[])    #Or should this be a lower bound as well???
#phi ~ dunif(lb, 0.9999)
}
",fill = TRUE)
sink()
library(R2jags)
# Create model. Try a basic Poisson first.
# due to n=100, lots of variability in the model.
# I usually use n=10000 for my synthetic models.
x1 <- runif(1000)
x2 <- runif(1000)
eta <- 1 - 0.4 * x1 - 0.4 * x2
mu <- exp(eta)
y <- rpois(1000,lambda=mu)
poi <- glm(y ~ x1 + x2, family = poisson)
summary(poi)
X <- model.matrix(~ x1 + x2)
K <- ncol(X)
win.data1 <- list(
Y    = y,
N    = 1000,
X    = X,
bet0 = rep(0, K),          #Betas count
Bet0 = diag(0.0001, K),   #Betas count
Zeros   = rep(0, 1000)
)
win.data1
########
#Model
sink("GPGLM.txt")
cat("
model{
#Priors regression parameters
beta  ~ dmnorm(bet0[], Bet0[,])
#########
#Likelihood
C <- 10000
phi <- 0
for (i in 1:N) {
#Log likelihood function of Generalized Poisson via zero trick:
Zeros[i] ~ dpois(Zeros.mean[i])
Zeros.mean[i] <- -L[i] + C
l1[i] <- log(theta[i])
l2[i] <- (Y[i] - 1) * log(theta[i] + phi * Y[i])
l3[i] <- (theta[i] + phi * Y[i])
l4[i] <- loggam(Y[i] + 1)
L[i]  <-  l1[i] + l2[i] - l3[i] - l4[i]
theta[i] <- (1 - phi) * exp(eta[i])
eta[i]   <- inprod(X[i,], beta[])
z[i]     <- max(-1, -theta[i] / 4)  #Condition
# theta[i] + phi * Y[i] must be >0..else the log is Inf
zz[i]    <- -Y[i]/theta[i]
}
lb <- max(z[])
lb2 <- min(zz[])
#phi ~ dunif(lb, 0.9999)
}
",fill = TRUE)
sink()
#######
#Inits function
inits1  <- function () {
list(beta  = rnorm(K, 0, 0.01)#,  #Regression parameters
#phi   = 0
)  }
#Parameters to estimate
params1 <- c("beta",
#"phi"
"lb",
"lb2"
)
#Start Gibbs sampler
GP   <- jags(data       = win.data1,
inits      = inits1,
parameters = params1,
model      = "GPGLM.txt",
n.thin     = 10,
n.chains   = 3,
n.burnin   = 5000,
n.iter     = 10000)
summary(GP)
GP
ggplot(pred.NB2err,aes(x=MBH,y=NGC))+
geom_ribbon(data=pred.NB2errx,aes(x=MBHx,y=mean,ymin=lwr1, ymax=upr1), alpha=0.3, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MBHx,y=mean,ymin=lwr2, ymax=upr2), alpha=0.2, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MBHx,y=mean,ymin=lwr3, ymax=upr3), alpha=0.1, fill="gray") +
geom_point(aes(colour=Type,shape=Type),size=3.25)+
geom_errorbar(guide="none",aes(colour=Type,ymin=NGC-N_err,ymax=NGC+N_err),alpha=0.7)+
geom_errorbarh(guide="none",aes(colour=Type,xmin=MBH-GCS$lowMBH,
xmax=MBH+upMBH),alpha=0.7)+
geom_line(data=pred.NB2errx,aes(x=MBHx,y=mean),colour="gray25",linetype="dashed",size=1.2)+
scale_y_continuous(trans = 'log10',breaks=trans_breaks("log10",function(x) 10^x),
labels=trans_format("log10",math_format(10^.x)))+
scale_colour_gdocs()+
scale_shape_manual(values=c(19,2,8))+
#  theme_economist_white(gray_bg = F, base_size = 11, base_family = "sans")+
theme_hc()+
ylab(expression(N[GC]))+
xlab(expression(log~M[BH]/M['\u0298']))+theme(legend.position="top",plot.title = element_text(hjust=0.5),
axis.title.y=element_text(vjust=0.75),
axis.title.x=element_text(vjust=-0.25),
text = element_text(size=25))
setwd("~/Dropbox/artigos/Meusartigos/IAA-WGC/Github/COINtoolbox/NB_GCs/Models_each_predictor")
#Poisson and NB regression using JAGS by Rafael S. de Souza, Bart Buelens, Ewan Cameron
#  Required libraries
library(rjags)
library(ggmcmc)
library(ggplot2)
library(ggthemes)
library(pander)
library(Cairo)
library(plyr)
library(MASS)
library(scales)
# Function to allow parse labels in facet_wrap
facet_wrap_labeller <- function(gg.plot,labels=NULL) {
#works with R 3.0.1 and ggplot2 0.9.3.1
require(gridExtra)
g <- ggplotGrob(gg.plot)
gg <- g$grobs
strips <- grep("strip_t", names(gg))
for(ii in seq_along(labels))  {
modgrob <- getGrob(gg[[strips[ii]]], "strip.text",
grep=TRUE, global=TRUE)
gg[[strips[ii]]]$children[[modgrob$name]] <- editGrob(modgrob,label=labels[ii])
}
g$grobs <- gg
class(g) = c("arrange", "ggplot",class(g))
g
}
give.n <- function(x){
return(c(y = 0.5, label = length(x)))
# experiment with the multiplier to find the perfect position
}
################
# Script starts here
# Read data
GCS = read.csv(file="..//Dataset//GCs.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(Mdyn)) # 1 removed
#dim(GCS)
N_err<-GCS$N_GC_err
lowMBH<-GCS$lowMBH
upMBH<-GCS$upMBH
#err_sig_e<-GCS$err_sig_e
######## NB with errors ########################################################
MBHx = seq(from = 0.95 * min(GCS$MBH),
to = 1.05 * max(GCS$MBH),
length.out = 100)
jags.data3 <- list(
=======
install.packages("assertthat")
install.packages("ggplot2")
install.packages("gsl")
install.packages("gsl",type="source")
data_path<-"/Users/rafael/Dropbox/artigos/Meusartigos/IAA-WGC/GLMs/Simulation/data/"
Biffi_data<-read.table(file=paste(data_path,"Biffi2014.csv",sep=""),
header=TRUE)
Biffi_data$fstar<-Biffi_data$Mstar/Biffi_data$Mdm
Biffi_data$fgas<-Biffi_data$Mgas/Biffi_data$Mdm
Biffi_data[1,]
Biffi_data$star.gas<-Biffi_data$Mstar/Biffi_data$Mgas
summary(Biffi_data)
Biffi_data<-read.table(file=paste(data_path,"Biffi2014.csv",sep=""),
header=TRUE)
#Biffi_data_original<-Biffi_data
# Problem 1: xmol, Z, SFR. SFR is the response variable
#Biffi_data<-Biffi_data[,c("SFR","Xmol","Z")]
Biffi_data$Mstar<-1e10*Biffi_data$Mstar
Biffi_data$SFR<-1e10*Biffi_data$SFR
Biffi_data$fstar<-Biffi_data$Mstar/Biffi_data$Mdm
Biffi_data$fgas<-Biffi_data$Mgas/Biffi_data$Mdm
Biffi_data$star.gas<-Biffi_data$Mstar/Biffi_data$Mgas
summary(Biffi_data)
sd(Biffi_data)
sd(Biffi_data$Mdm)
mean(Biffi_data$Mdm)
sd(Biffi_data$Mdm)
sd(Biffi_data$Mgas)
sd(Biffi_data$Mstar)
sd(Biffi_data$SFR)
sd(Biffi_data$Z)
sd(Biffi_data$xmol)
sd(Biffi_data$Xmol)
sd(Biffi_data$fgas)
sd(Biffi_data$fstar)
sd(Biffi_data$star.gas)
install.packages("boot")
install.packages("arm")
install.packages("reshape2")
install.packages("reshape")
install.packages("plyr")
install.packages("COUNT")
install.packages("MCMCpack")
install.packages("stargazer")
install.packages("pROC")
install.packages("scales")
install.packages("SDMTools")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
exp(5.9)
365/(1+365)
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cdoc <- R84$docvis
### JAGS
### ======================================================
K <- 1
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
cdoc   = R84$cdoc,
K=1,
LogN = log(nrow(R84))
)
>>>>>>> Stashed changes
sink("GLM.txt")
cat("
model{
#1. Priors
<<<<<<< Updated upstream
#    beta  ~ dmnorm(b0[], B0[,])
beta  ~ dt(b0[], B0[],nu[])
=======
beta.0 ~ dt(0, 1/10^2, 1)
beta.1~dt(0, 1, 1)
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- beta.0+beta.1*cdoc[i]
#    cdoc[i]~dbeta(34,14.5)
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta.0  = 0.1,beta.1=0.1
)  }
params <- c("beta.0","beta.1","LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
J0
1/2.5^2
2.5
1/(2.5^2)
cat("
model{
#1. Priors
beta.0 ~ dt(0, 0.16, 1)
beta.1~dt(0, 0.16, 1)
>>>>>>> Stashed changes
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
<<<<<<< Updated upstream
eta[i]      <- inprod(beta[], X[i,])
=======
eta[i]      <- beta.0+beta.1*cdoc[i]
#    cdoc[i]~dbeta(34,14.5)
>>>>>>> Stashed changes
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
<<<<<<< Updated upstream
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta", "LogL", "AIC", "BIC")
=======
beta.0  = 0.1,beta.1=0.1
)  }
params <- c("beta.0","beta.1","LogL", "AIC", "BIC")
>>>>>>> Stashed changes
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
<<<<<<< Updated upstream
rep(1/(2.5^2), K)
rep(1/(2.5^2), 5)
rep(2,5)
1+4
1+1
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '2g')
library(h2o)
install.packages("h2o")
library(h2o)
## Start a local cluster with 2GB RAM
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '2g')
dat <- BreastCancer[, -1]  # remove the ID column
dat_h2o <- as.h2o(localH2O, dat, key = 'dat')
## Import MNIST CSV as H2O
dat_h2o <- h2o.importFile(localH2O, path = ".../mnist_train.csv")
dat_h2o
dat
BreastCancer[, -1]
localH2O
library(h2o)
localH2O = h2o.init()
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
data.hex = h2o.importFile(
localH2O,
path = "https://raw.github.com/0xdata/h2o/master/smalldata/bank-additional-full.csv",
key = "data.hex")
myX = 1:20
myY="y"
my.dl = h2o.deeplearning(x=myX,y=myY,data=data.hex,classification=T,activation="Tanh",
hidden=c(10,10,10),epochs=12,variable_importances=T)
dl.VI =my.dl@model$varimp
print(dl.VI)
dl.VI
iris.hex
localH2O
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '16g')
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g')
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=4)
localH2O
h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=4)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=-1)
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=-1)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '32g',nthreads=-1)
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
setwd("~/Dropbox/artigos/Meusartigos/IAA-WGC/Github/NB_GCs/Models_each_predictor")
#Poisson and NB regression using JAGS by Rafael S. de Souza, Bart Buelens, Ewan Cameron
#  Required libraries
library(rjags)
library(ggmcmc)
library(ggplot2)
library(ggthemes)
library(pander)
library(Cairo)
library(plyr)
library(MASS)
library(scales)
# Function to allow parse labels in facet_wrap
facet_wrap_labeller <- function(gg.plot,labels=NULL) {
#works with R 3.0.1 and ggplot2 0.9.3.1
require(gridExtra)
g <- ggplotGrob(gg.plot)
gg <- g$grobs
strips <- grep("strip_t", names(gg))
for(ii in seq_along(labels))  {
modgrob <- getGrob(gg[[strips[ii]]], "strip.text",
grep=TRUE, global=TRUE)
gg[[strips[ii]]]$children[[modgrob$name]] <- editGrob(modgrob,label=labels[ii])
}
=======
J0
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(0.00001, K)
)
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cage <- R84$age - mean(R84$age)
R84$cdoc <- R84$docvis - mean(R84$docvis)
X <- model.matrix(~ cdoc + female + kids + cage,
data = R84)
K <- ncol(X)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(0.00001, K)
)
sink("GLM.txt")
diag(0.00001, K)
diag(0.00001, 5)
B0   = diag(0.00001, K)
B0
diag(0.00001, 3)
nu = rep(1,K)
nu[]
library(R2jags)
library(COUNT)
data(rwm1984)
R84 <- rwm1984
R84$cage <- R84$age - mean(R84$age)
R84$cdoc <- R84$docvis - mean(R84$docvis)
X <- model.matrix(~ cdoc + female + kids + cage,
data = R84)
K <- ncol(X)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = diag(1/(2.5^2), K),
nu = rep(1,K)
)
sink("GLM.txt")
cat("
model{
#1. Priors
#    beta  ~ dmnorm(b0[], B0[,])
beta  ~ dt(b0[], B0[,],nu[])
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- inprod(beta[], X[i,])
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta", "LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
win.data <- list(Y    = R84$outwork,
N    = nrow(R84),
X    = X,
K    = K,
LogN = log(nrow(R84)),
b0   = rep(0, K),
B0   = rep(1/(2.5^2), K),
nu = rep(1,K)
)
sink("GLM.txt")
cat("
model{
#1. Priors
#    beta  ~ dmnorm(b0[], B0[,])
beta  ~ dt(b0[], B0[],nu[])
#2. Likelihood
for (i in 1:N){
Y[i] ~ dbern(p[i])
logit(p[i]) <- max(-20, min(20, eta[i]))
eta[i]      <- inprod(beta[], X[i,])
LLi[i] <- Y[i] * log(p[i]) +
(1 - Y[i]) * log(1 - p[i])
}
LogL <- sum(LLi[1:N])
AIC <- -2 * LogL + 2 * K
BIC <- -2 * LogL + LogN * K
}
",fill = TRUE)
sink()
# INITIAL VALUES - BETAS AND SIGMAS
inits <- function () {
list(
beta  = rnorm(K, 0, 0.1)
)  }
params <- c("beta", "LogL", "AIC", "BIC")
# JAGs
J0 <- jags(data = win.data,
inits = inits,
parameters = params,
model.file = "GLM.txt",
n.thin = 10,
n.chains = 3,
n.burnin = 40000,
n.iter   = 50000)
rep(1/(2.5^2), K)
rep(1/(2.5^2), 5)
rep(2,5)
1+4
1+1
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '2g')
library(h2o)
install.packages("h2o")
library(h2o)
## Start a local cluster with 2GB RAM
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '2g')
dat <- BreastCancer[, -1]  # remove the ID column
dat_h2o <- as.h2o(localH2O, dat, key = 'dat')
## Import MNIST CSV as H2O
dat_h2o <- h2o.importFile(localH2O, path = ".../mnist_train.csv")
dat_h2o
dat
BreastCancer[, -1]
localH2O
library(h2o)
localH2O = h2o.init()
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
data.hex = h2o.importFile(
localH2O,
path = "https://raw.github.com/0xdata/h2o/master/smalldata/bank-additional-full.csv",
key = "data.hex")
myX = 1:20
myY="y"
my.dl = h2o.deeplearning(x=myX,y=myY,data=data.hex,classification=T,activation="Tanh",
hidden=c(10,10,10),epochs=12,variable_importances=T)
dl.VI =my.dl@model$varimp
print(dl.VI)
dl.VI
iris.hex
localH2O
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '16g')
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g')
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=4)
localH2O
h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=4)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=-1)
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '16g',nthreads=-1)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
max_mem_size = '32g',nthreads=-1)
irisPath = system.file("extdata", "iris.csv", package = "h2o")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = "Tanh",
hidden = c(10, 10), epochs = 5)
setwd("~/Dropbox/artigos/Meusartigos/IAA-WGC/Github/NB_GCs/Models_each_predictor")
#Poisson and NB regression using JAGS by Rafael S. de Souza, Bart Buelens, Ewan Cameron
#  Required libraries
library(rjags)
library(ggmcmc)
library(ggplot2)
library(ggthemes)
library(pander)
library(Cairo)
library(plyr)
library(MASS)
library(scales)
# Function to allow parse labels in facet_wrap
facet_wrap_labeller <- function(gg.plot,labels=NULL) {
#works with R 3.0.1 and ggplot2 0.9.3.1
require(gridExtra)
g <- ggplotGrob(gg.plot)
gg <- g$grobs
strips <- grep("strip_t", names(gg))
for(ii in seq_along(labels))  {
modgrob <- getGrob(gg[[strips[ii]]], "strip.text",
grep=TRUE, global=TRUE)
gg[[strips[ii]]]$children[[modgrob$name]] <- editGrob(modgrob,label=labels[ii])
}
>>>>>>> Stashed changes
g$grobs <- gg
class(g) = c("arrange", "ggplot",class(g))
g
}
give.n <- function(x){
return(c(y = 0.5, label = length(x)))
# experiment with the multiplier to find the perfect position
}
################
# Script starts here
# Read data
GCS = read.csv(file="..//Dataset//GCs_full.csv",header=TRUE,dec=".",sep="")
GCS = subset(GCS, !is.na(MV_T))
#dim(GCS)
N_err<-GCS$N_GC_err
err_MV_T<-GCS$err_MV_T
type<-match(GCS$Type,unique(GCS$Type))
Ntype<-length(unique(GCS$Type))
######## NB with errors ########################################################
MV_Tx = seq(from = 1.05 * min(GCS$MV_T),
to = 0.95 * max(GCS$MV_T),
length.out = 1000)
jags.data <- list(
>>>>>>> origin/master
N_GC = GCS$N_GC,
MBH = GCS$MBH,
errN_GC = GCS$N_GC_err,
N = nrow(GCS),
errMBH = upMBH,
MBHx = MBHx,
M = 100
)
model.NB <- "model{
# Priors for regression coefficients
beta.0~dnorm(0,0.000001)
beta.1~dnorm(0,0.000001)
<<<<<<< HEAD
=======
beta.2~dnorm(0,0.000001)
tau.R~dgamma(0.01,0.01)
<<<<<<< Updated upstream
=======
>>>>>>> origin/master
>>>>>>> Stashed changes
# Prior for size
size~dunif(0.001,5)
# Hyperpriors
#meanx ~ dgamma(30,3)
#varx ~ dgamma(2,1)
meanx ~ dgamma(85,10)
varx ~ dgamma(2,1)
for (i in 1:N){
#MBHtrue[i]~dunif(5,12)
# MBHtrue[i]~dnorm(8,0.000001) # this would be sensible too
MBHtrue[i] ~ dgamma(meanx^2/varx,meanx/varx)T(6,11)
}
# Likelihood function
for (i in 1:N){
MBH[i]~dnorm(MBHtrue[i],1/errMBH[i]^2);
errorN[i]~dbin(0.5,2*errN_GC[i])
eta[i]<-beta.0+beta.1*MBHtrue[i]+exp(errorN[i]-errN_GC[i])
log(mu[i])<-max(-20,min(20,eta[i]))# Ensures that large beta values do not cause numerical problems.
p[i]<-size/(size+mu[i])
N_GC[i]~dnegbin(p[i],size)
# Prediction
etaTrue[i]<-beta.0+beta.1*MBHtrue[i]
log(muTrue[i])<-max(-20,min(20,etaTrue[i]))
pTrue[i]<-size/(size+muTrue[i])
prediction.NB[i]~dnegbin(pTrue[i],size)
#prediction.NB[i]~dnegbin(p[i],size)
# Discrepancy measures
YNew[i] ~ dnegbin(p[i],size)
expY[i] <- mu[i]
varY[i] <- mu[i] + pow(mu[i],2) / size
PRes[i] <-(N_GC[i] - expY[i])/sqrt(varY[i])
PResNew[i] <-(YNew[i] - expY[i])/sqrt(varY[i])
D[i]<-pow(PRes[i],2)
DNew[i]<-pow(PResNew[i],2)
}
Fit<-sum(D[1:N])
New<-sum(DNew[1:N])
# Prediction for new data
for (j in 1:M){
etax[j]<-beta.0+beta.1*MBHx[j]
log(mux[j])<-max(-20,min(20,etax[j]))
px[j]<-size/(size+mux[j])
prediction.NBx[j]~dnegbin(px[j],size)
}
}"
<<<<<<< HEAD
inits3 <- list(beta.0=0,beta.1=0,size=0.1)
params3 <- c("beta.0","beta.1","size","prediction.NB","MBHtrue","Fit","New","prediction.NBx")
jags.neg3 <- jags.model(
data = jags.data3,
inits = inits3,
=======
inits <- list(beta.0=0,beta.1=0,size=0.1)
params <- c("beta.0","beta.1","size","ranef","prediction.NB","MV_T_true","Fit","New","prediction.NBx")
jags.neg <- jags.model(
data = jags.data,
inits = inits,
>>>>>>> origin/master
textConnection(model.NB),
n.chains = 3,
n.adapt=1000
)
update(jags.neg3, 10000)
jagssamples.nb3 <- jags.samples(jags.neg3, params3, n.iter = 50000)
codasamples.nb3 <- coda.samples(jags.neg3, params3, n.iter = 50000)
dicsamples.nb3 <- dic.samples(jags.neg3, params3, n.iter = 50000,type="pD")
summary(as.mcmc.list(jagssamples.nb3$beta.0))
summary(as.mcmc.list(jagssamples.nb3$beta.1))
summary(as.mcmc.list(jagssamples.nb3$size))
MBHtrue<-summary(as.mcmc.list(jagssamples.nb3$MBHtrue),quantiles=0.5)
pred.NBerr<-summary(as.mcmc.list(jagssamples.nb3$prediction.NB),quantiles=c(0.005,0.025,0.25,0.5,0.75,0.975, 0.995))
pred.NB2err<-data.frame(Type=GCS$Type,NGC=GCS$N_GC,MBHtrue=MBHtrue$quantiles,MBH=GCS$MBH,mean=pred.NBerr$statistics[,1],lwr1=pred.NBerr$quantiles[,3],lwr2=pred.NBerr$quantiles[,2],lwr3=pred.NBerr$quantiles[,1],upr1=pred.NBerr$quantiles[,5],upr2=pred.NBerr$quantiles[,6],upr3=pred.NBerr$quantiles[,7])
pred.NBerrx<-summary(as.mcmc.list(jagssamples.nb3$prediction.NBx),quantiles=c(0.005,0.025,0.25,0.5,0.75,0.975, 0.995))
pred.NB2errx<-data.frame(MBHx=MBHx,mean=pred.NBerrx$statistics[,1],lwr1=pred.NBerrx$quantiles[,3],lwr2=pred.NBerrx$quantiles[,2],lwr3=pred.NBerrx$quantiles[,1],upr1=pred.NBerrx$quantiles[,5],upr2=pred.NBerrx$quantiles[,6],upr3=pred.NBerrx$quantiles[,7])
require(aod)  # to get Crowder seed data
require(ggplot2)
require(rjags)
data(orob2)
orob2$proportion  <- orob2$y/orob2$n
orob2$plate <- 1: nrow(orob2)
orob2$group <- paste(orob2$root," : ", orob2$seed,sep='')
model <- factor(c('Logistic Model','Observed','Random Effect'))
seedplot <- ggplot(data=orob2, aes( x = plate,
y = proportion,
shape=group,
colour=factor('Observed'))) +
geom_point(size=3) +
scale_x_continuous("Plate") +
scale_y_continuous("Germination Proportion") +
scale_shape_discrete(name = "Root Extract:Seed",
breaks=unique(orob2$group)[c(1,2,3,4)]) +
scale_colour_manual(name='Observation/Prediction',
values=c('Logistic Model'='blue',
'Observed' = 'black',
'Random Effect' = 'green4'),
breaks=model)
seedplot
n <- 50
sdx <- 6
sdobs <- 5
taux <- 1 / (sdobs * sdobs)
truex <- rnorm(n, 0, sdx)
errorx <- rnorm(n, 0, sdobs)
obsx <- truex + errorx
# simulate response data
alpha <- 0
beta <- 10
sdy <- 20
errory <- rnorm(n, 0, sdy)
obsy <- alpha + beta*truex + errory
parms <- data.frame(alpha, beta)
jags_d <- list(x = obsx, y = obsy, n = length(obsx))
# write model
cat("
model{
## Priors
alpha ~ dnorm(0, .001)
beta ~ dnorm(0, .001)
sdy ~ dunif(0, 100)
tauy <- 1 / (sdy * sdy)
## Likelihood
for (i in 1:n){
mu[i] <- alpha + beta * x[i]
y[i] ~ dnorm(mu[i], tauy)
}
<<<<<<< Updated upstream
=======
<<<<<<< HEAD
",
fill=TRUE, file="yerror.txt")
require(rjags)
# initiate model
mod1 <- jags.model("yerror.txt", data=jags_d,
n.chains=3, n.adapt=1000)
# simulate posterior
out <- coda.samples(mod1, n.iter=1000, thin=1,
variable.names=c("alpha", "beta", "sdy"))
# store parameter estimates
require(ggmcmc)
ggd <- ggs(out)
a <- ggd$value[which(ggd$Parameter == "alpha")]
b <- ggd$value[which(ggd$Parameter == "beta")]
d <- data.frame(a, b)
# specify model
cat("
model {
## Priors
alpha ~ dnorm(0, .001)
beta ~ dnorm(0, .001)
sdy ~ dunif(0, 100)
tauy <- 1 / (sdy * sdy)
taux ~ dunif(.03, .05)
## Likelihood
for (i in 1:n){
truex[i] ~ dnorm(0, .04)
x[i] ~ dnorm(truex[i], taux)
mu[i] <- alpha + beta * truex[i]
y[i] ~ dnorm(mu[i], tauy)
}
", fill=T, file="xyerror.txt")
# bundle data
jags_d <- list(x = obsx, y = obsy, n = length(obsx))
# initiate model
mod2 <- jags.model("xyerror.txt", data=jags_d,
n.chains=3, n.adapt=1000)
# simulate posterior
out <- coda.samples(mod2, n.iter=30000, thin=30,
variable.names=c("alpha", "beta", "tauy", "taux"))
# store parameter estimates
ggd <- ggs(out)
a2 <- ggd$value[which(ggd$Parameter == "alpha")]
b2 <- ggd$value[which(ggd$Parameter == "beta")]
d2 <- data.frame(a2, b2)
ggplot(d, aes(x=obsx, obsy)) +
geom_abline(aes(intercept=a, slope=b), data=d, color="red", alpha=0.05) +
geom_abline(aes(intercept=a2, slope=b2), data=d2, color="blue", alpha=0.05) +
geom_abline(aes(intercept=alpha, slope=beta),
data=parms, color="green", size=1.5, linetype="dashed") +
theme_bw() +
geom_point(shape=1, size=3) +
xlab("X values") + ylab("Observed Y values") +
ggtitle("Model results with and without modeling error in X")
library(rCharts)
library(reshape2)
library(minerva)
findata=iris[,c(1:3)] # removing first dummy column - the csv quirk - second column on Rank, and third column on School. Retaining only numeric vars here
corrmatrix<-mine(findata)$MIC #store corr matrix
# The following steps are generic and can all be placed in a function with some tweaks to customize output
corrdata=as.data.frame(corrmatrix)
corrdata$Variable1=names(corrdata)
corrdatamelt=melt(corrdata,id="Variable1")
names(corrdatamelt)=c("Variable1","Variable2","CorrelationCoefficient")
corrmatplot = rPlot(Variable2 ~ Variable1, color = 'CorrelationCoefficient', data = corrdatamelt, type = 'tile', height = 600)
corrmatplot$addParams(height = 400, width=800)
corrmatplot$guides("{color: {scale: {type: gradient2, lower: 'red',  middle: 'white', upper: 'blue',midpoint: 0}}}")
corrmatplot$guides(y = list(numticks = length(unique(corrdatamelt$Variable1))))
corrmatplot$guides(x = list(numticks = 3))
#corrmatplot$addParams(staggerLabels=TRUE)
corrmatplot
X=c(1,2,3,4)
Y=c(1,2,5,6)
base=data.frame(X,Y)
reg1=lm(Y~1+X,data=base)
nbase=data.frame(X=seq(0,5,by=.1))
Y1=predict(reg1,newdata=nbase)
plot(X,Y,pch=3,cex=1.5,lwd=2,xlab="",ylab="")
#qplot(X,Y)
lines(nbase$X,Y1,col="red",lwd=2)
u=2
mu=predict(reg1)[2]
sigma=summary(reg1)$sigma
y=seq(0,7,.05)
loi=dpois(round(y),lambda=1)
segments(u,y,loi+u,y,col="light green")
# lines(loi+u,y)
abline(v=u,lty=2)
points(X[2],Y[2],pch=3,cex=1.5,lwd=2)
points(X[2],predict(reg1)[2],pch=19,col="red")
arrows(u-.2,qnorm(.05,mu,sigma),
+ u-.2,qnorm(.95,mu,sigma),length=0.1,code=3,col="blue")
=======
>>>>>>> Stashed changes
ggplot(pred.NB2err,aes(x=MV_T,y=NGC))+
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr1, ymax=upr1), alpha=0.4, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr2, ymax=upr2), alpha=0.3, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr3, ymax=upr3), alpha=0.2, fill="gray") +
geom_point(aes(colour=Type,shape=Type),size=3.25,alpha=0.7)+
geom_errorbar(guide="none",aes(colour=Type,ymin=NGC-N_low,ymax=NGC+N_err),alpha=0.7)+
geom_errorbarh(guide="none",aes(colour=Type,xmin=MV_T-GCS$err_MV_T,
xmax=MV_T+err_MV_T),alpha=0.7)+
geom_line(data=pred.NB2errx,aes(x=MV_Tx,y=mean),colour="gray25",linetype="dashed",size=1.2)+
scale_y_continuous(trans = 'asinh',breaks=c(0,10,100,1000,10000,100000),labels=c("0",expression(10^1),expression(10^2),
expression(10^3),expression(10^4),expression(10^5)))+
scale_colour_gdocs()+
scale_shape_manual(values=c(19,2,8,10))+scale_x_reverse()+
#  theme_economist_white(gray_bg = F, base_size = 11, base_family = "sans")+
theme_hc()+
ylab(expression(N[GC]))+
xlab(expression(M[V]))+theme(legend.position="top",plot.title = element_text(hjust=0.5),
axis.title.y=element_text(vjust=0.75),
axis.title.x=element_text(vjust=-0.25),
text = element_text(size=25))
cairo_pdf("..//Figures/M_Vx_random.pdf",height=8,width=9)
ggplot(pred.NB2err,aes(x=MV_T,y=NGC))+
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr1, ymax=upr1), alpha=0.4, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr2, ymax=upr2), alpha=0.3, fill="gray") +
geom_ribbon(data=pred.NB2errx,aes(x=MV_Tx,y=mean,ymin=lwr3, ymax=upr3), alpha=0.2, fill="gray") +
geom_point(aes(colour=Type,shape=Type),size=3.25,alpha=0.7)+
geom_errorbar(guide="none",aes(colour=Type,ymin=NGC-N_low,ymax=NGC+N_err),alpha=0.7)+
geom_errorbarh(guide="none",aes(colour=Type,xmin=MV_T-GCS$err_MV_T,
xmax=MV_T+err_MV_T),alpha=0.7)+
geom_line(data=pred.NB2errx,aes(x=MV_Tx,y=mean),colour="gray25",linetype="dashed",size=1.2)+
scale_y_continuous(trans = 'asinh',breaks=c(0,10,100,1000,10000,100000),labels=c("0",expression(10^1),expression(10^2),
expression(10^3),expression(10^4),expression(10^5)))+
scale_colour_gdocs()+
scale_shape_manual(values=c(19,2,8,10))+scale_x_reverse()+
#  theme_economist_white(gray_bg = F, base_size = 11, base_family = "sans")+
theme_hc()+
ylab(expression(N[GC]))+
xlab(expression(M[V]))+theme(legend.position="top",plot.title = element_text(hjust=0.5),
axis.title.y=element_text(vjust=0.75),
axis.title.x=element_text(vjust=-0.25),
text = element_text(size=25))
dev.off()
<<<<<<< Updated upstream
=======
>>>>>>> origin/master
>>>>>>> Stashed changes
